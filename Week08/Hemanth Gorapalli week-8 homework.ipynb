{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a8bd498",
   "metadata": {},
   "source": [
    "# Week8 - Homework KNN-NB-SVM\n",
    "\n",
    "- Use GridSearchCV on X_train dataset\n",
    "    - KNN, NB, SVM, Logistic Regression, Decision Trees\n",
    "- Test on X_test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffd31a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5f84b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('https://github.com/msaricaumbc/DS_data/raw/master/ds602/movie/X_train.csv')\n",
    "y_train = pd.read_csv('https://github.com/msaricaumbc/DS_data/raw/master/ds602/movie/y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d1b1fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('https://github.com/msaricaumbc/DS_data/raw/master/ds602/movie/X_final.csv')\n",
    "y_test = pd.read_csv('https://github.com/msaricaumbc/DS_data/raw/master/ds602/movie/y_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6388d42b",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis(EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2e6567",
   "metadata": {},
   "source": [
    "Here we are using EDA  because we dont know how the data will be and we dont how the data will be and how can we able to \n",
    "perform data cleaning with the help of this analysis we will be able to identify it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d42a4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for the train data containing the number of rows and columns\n",
      "X_train data: (40000, 1)\n",
      "y_train data: (40000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking for the train data containing the number of rows and columns\")\n",
    "print(\"X_train data:\", X_train.shape)\n",
    "print(\"y_train data:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19ec3c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for the test data containing number of rows and columns\n",
      "X_test data: (10000, 1)\n",
      "y_test data: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking for the test data containing number of rows and columns\")\n",
    "print(\"X_test data:\", X_test.shape)\n",
    "print(\"y_test data:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46282204",
   "metadata": {},
   "source": [
    "Checking for the training data to show how the data was represented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "346a2212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data in X_train:\n",
      "                                              review\n",
      "0  Shame, is a Swedish film in Swedish with Engli...\n",
      "1  I know it's rather unfair to comment on a movi...\n",
      "2  \"Bread\" very sharply skewers the conventions o...\n",
      "3  After reading tons of good reviews about this ...\n",
      "4  During the Civil war a wounded union soldier h...\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample data in X_train:\")\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "380c57aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data of y_train:\n",
      "   sentiment\n",
      "0          1\n",
      "1          0\n",
      "2          1\n",
      "3          1\n",
      "4          1\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample data of y_train:\")\n",
    "print(y_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8f6a50",
   "metadata": {},
   "source": [
    "Checking for the test data to show how the data was represented:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23606f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data in X_test:\n",
      "                                              review\n",
      "0  I first saw Heimat 2 on BBC2 in the 90's when ...\n",
      "1  I sat down to watch \"Midnight Cowboy\" thinking...\n",
      "2  I can never fathom why people take time to rev...\n",
      "3  With that line starts one silly, boring Britis...\n",
      "4  Here's the spoiler: At the end of the movie, a...\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample data in X_test:\")\n",
    "print(X_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f57aaf07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data in y_test:\n",
      "   sentiment\n",
      "0          1\n",
      "1          1\n",
      "2          1\n",
      "3          0\n",
      "4          0\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample data in y_test:\")\n",
    "print(y_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a5f7fc",
   "metadata": {},
   "source": [
    "To check any Null Values or any values are missing in the data we are using isnul().sum() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca3958be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missing values present in X_train:\n",
      " review    0\n",
      "dtype: int64\n",
      "Number of missing values present in y_train:\n",
      " sentiment    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of missing values present in X_train:\\n\", X_train.isnull().sum())\n",
    "print(\"Number of missing values present in y_train:\\n\", y_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7927997b",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8acb46db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here by squeeze method we are converting them into 1 dimensional arrays\n",
    "y_train = y_train.squeeze()\n",
    "y_test = y_test.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b39f170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to large data we are going to take less sample size\n",
    "subset_size = 0.1  \n",
    "X_train, _, y_train, _ = train_test_split(X_train, y_train, train_size=subset_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45d9e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder_y = LabelEncoder()\n",
    "y_train_encoded = label_encoder_y.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder_y.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11e2d15",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26a5551",
   "metadata": {},
   "source": [
    "In this data set as we see we dont have numerical and categorical data with the help of vectorizer you will be able to get better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38cb3cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the pipelines and parameter grids\n",
    "pipelines = {\n",
    "    'Logistic Regression': Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer()),\n",
    "        ('clf', LogisticRegression())\n",
    "    ]),\n",
    "    'SVM': Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer()),\n",
    "        ('clf', SVC())\n",
    "    ]),\n",
    "    'KNN': Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer()),\n",
    "        ('clf', KNeighborsClassifier())\n",
    "    ]),\n",
    "    'Naive Bayes': Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer()),\n",
    "        ('clf', MultinomialNB())\n",
    "    ]),\n",
    "    'Decision Tree': Pipeline([\n",
    "        ('vectorizer', TfidfVectorizer()),\n",
    "        ('clf', DecisionTreeClassifier())\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c1421f",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4e10630",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'clf__C': [0.1, 1, 10]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'clf__C': [0.1, 1, 10],\n",
    "        'clf__gamma': [0.01, 0.1, 1]\n",
    "    },\n",
    "    'KNN': {\n",
    "        'clf__n_neighbors': [3, 5, 7, 9]\n",
    "    },\n",
    "    'Naive Bayes': {},\n",
    "    'Decision Tree': {\n",
    "        'clf__max_depth': [None, 10, 20]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9ecfb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search for Every Model \n",
    "results = {}\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    param_grid = param_grids[model_name]\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=3, n_jobs=-1)\n",
    "    grid_search.fit(X_train.squeeze(), y_train_encoded)\n",
    "    results[model_name] = grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8ee138b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - Test Accuracy: 0.8641\n",
      "\n",
      "SVM - Test Accuracy: 0.8662\n",
      "\n",
      "KNN - Test Accuracy: 0.6924\n",
      "\n",
      "Naive Bayes - Test Accuracy: 0.8355\n",
      "\n",
      "Decision Tree - Test Accuracy: 0.7054\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model test accuracy check with parameters\n",
    "for model_name, grid_search in results.items():\n",
    "    accuracy = grid_search.score(X_test.squeeze(), y_test_encoded)\n",
    "    print(f'{model_name} - Test Accuracy: {accuracy}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861593fb",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b0e885",
   "metadata": {},
   "source": [
    "I have observed that Logistic Regression and SVM is dominating the test accuracy \n",
    "when compared to other models and Bayes also good too \n",
    "and i was dissapointed on knn because it has least accuracy when compared to other models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
